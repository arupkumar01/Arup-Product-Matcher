// backend/utils/cleanup.js
import fs from "fs";
import path from "path";
import mongoose from "mongoose";
import dotenv from "dotenv";
import Product from "../models/Product.js";

dotenv.config();

const folder = path.resolve("./uploads");
const cutoff = Date.now() - 24 * 60 * 60 * 1000; // older than 1 day

async function main() {
  console.log("ðŸ§¹ Starting cleanup job...");

  // Check if uploads folder exists
  if (!fs.existsSync(folder)) {
    console.log("âš ï¸ Upload folder not found â€” skipping cleanup.");
    return;
  }

  // Connect to MongoDB
  const MONGO_URI = process.env.MONGO_URI || "mongodb://127.0.0.1:27017/visual_matcher";
  await mongoose.connect(MONGO_URI, {
    useNewUrlParser: true,
    useUnifiedTopology: true,
  });

  console.log("âœ… Connected to MongoDB for cleanup.");

  const files = fs.readdirSync(folder);
  const products = await Product.find({}, { image: 1 });
  const protectedPaths = new Set(products.map((p) => path.basename(p.image)));

  let deletedCount = 0;
  let skippedCount = 0;

  for (const file of files) {
    try {
      const filePath = path.join(folder, file);
      const stats = fs.statSync(filePath);

      // Skip if file is still referenced by MongoDB
      if (protectedPaths.has(file)) {
        skippedCount++;
        continue;
      }

      // Delete if older than cutoff
      if (stats.mtimeMs < cutoff) {
        fs.unlinkSync(filePath);
        console.log(`ðŸ—‘ï¸ Deleted old file: ${file}`);
        deletedCount++;
      }
    } catch (err) {
      console.error(`âŒ Error processing ${file}:`, err.message);
    }
  }

  console.log(`\nðŸ“Š Cleanup Summary:
   âœ… Checked: ${files.length} files
   ðŸ—‘ï¸ Deleted: ${deletedCount}
   â­ï¸ Skipped (still in DB): ${skippedCount}`);

  await mongoose.disconnect();
  console.log("ðŸ”Œ MongoDB disconnected. Cleanup complete.\n");
}

main().catch((err) => {
  console.error("ðŸš¨ Fatal error in cleanup:", err);
  process.exit(1);
});
